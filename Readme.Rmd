---
output: github_document
---

```{r, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

## Create Test Datasets

```{r}
set.seed(314159)

df.train = iris[sample(nrow(iris)), ]
idx.test = sample(x = seq_len(nrow(df.train)), size = 0.1 * nrow(df.train))

df.test  = df.train[idx.test, ]
df.train = df.train[-idx.test, ]

splits = 3L
breaks = seq(1, nrow(df.train), length.out = splits + 1)

# Save single train sets:
files = character(splits)
for (i in seq_len(splits)) {
	files[i] = paste0("data/iris", i, ".csv")

	temp = df.train[breaks[i]:breaks[i + 1], ]
	write.csv(x = temp, file = files[i], row.names = FALSE)
}

# Save test set:
write.csv(x = df.test, file = "data/iris_test.csv")
```


## Run Gradient Descent 

First of all, load the package using `devtools`:

```{r}
devtools::load_all()
```

In the next step we create an `Model` object to specify which model we want to fit using a specific optimizer:

```{r}
X = cbind(1, 1:10)
y = 1:10

lin.mod = LinearModel$new(X, y)
lin.mod

lin.mod$calculateMSE(1:2)

( lin.mod.ptr = useLinearModel(X, y) )
testLinearModel(lin.mod.ptr, c(1,2))
lin.mod.ptr

gc()

lin.mod.ptr
testLinearModel(lin.mod.ptr, c(1,2))
```


```{r}
myformula = formula(Sepal.Length ~ Petal.Length + Sepal.Width)

mod.lm = lm(myformula, data = df.train)
summary(mod.lm)

mybeta = lmGradientDescent(myformula, data = df.train, iters = 30000L, learning_rate = 0.01,
	mse_eps = 1e-10, trace = FALSE)
```
```{r, echo=FALSE}
knitr::kable(data.frame(lm = coef(mod.lm), grad.descent = mybeta$beta, diff = coef(mod.lm) - mybeta$beta))
```

### Doing just one step

```{r}
actual_beta = coef(mod.lm) * 1.1
mybeta = updateBeta (myformula, data = df.train, learning_rate = 0.01, actual_beta = actual_beta, 
  mse_eps = 1e-10, trace = FALSE, warnings = FALSE)

knitr::kable(data.frame(lm = coef(mod.lm), actual.step = actual_beta, after.step = mybeta$beta))
```

## Distributed Linear Model

```{r}
source("R/distributed_lm.R")
```

Initialize model:

```{r}
files = c("data/iris1.csv", "data/iris2.csv", "data/iris3.csv")
myformula = formula(Sepal.Length ~ Petal.Length + Sepal.Width)

initializeDistributedLinearModel(formula = myformula, out_dir = getwd(), files = files, epochs = 30000L, 
	learning_rate = 0.01, mse_eps = 1e-10, file_reader = read.csv, overwrite = TRUE)
```

We can have a look at the created registry:
```{r}
load("train_files/registry.rds")
registry

load("train_files/model.rds")
model
```

This file is permanent and holds all necessary data to coordinate the fitting process of the train.

Now we can train a linear model by loading the files specified in files sequentially:

```{r}
trainDistributedLinearModel(regis = "train_files")
trainDistributedLinearModel(regis = "train_files")

load("train_files/registry.rds")
registry

load("train_files/model.rds")
model
```

We can now train the model until the stopping criteria hits, which is ether the maximal number of epochs or the relative improvement of the MSE is smaller then the specified epsilon `r registry[["mse_eps"]]`: 

```{r}
while(! model[["done"]]) {
	trainDistributedLinearModel(regis = "train_files", silent = TRUE)
	load("train_files/model.rds")
}
```

We can have a final look on the estimated parameter by loading the final model:

```{r}
load("train_files/registry.rds")
registry[["actual_iteration"]]

load("train_files/model.rds")
model

knitr::kable(data.frame(lm = coef(mod.lm), actual.step = model[["beta"]]), diff = coef(mod.lm) - model[["beta"]])
```