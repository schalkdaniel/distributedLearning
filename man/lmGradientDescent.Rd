% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/grad_descent_lm.R
\name{lmGradientDescent}
\alias{lmGradientDescent}
\title{Train a linear model using gradient descent}
\usage{
lmGradientDescent(formula, data, iters = 100L, learning_rate = 0.05,
  beta_init = NULL, mse_eps = 1e-06, trace = FALSE, warnings = FALSE)
}
\arguments{
\item{formula}{[\code{formula}]\cr
Formula analog to the formula call in \code{lm}.}

\item{data}{[\code{data.frame}]\cr
Data frame containing the data used for modeling.}

\item{iters}{[\code{integer(1)}]\cr
Number of maximal iterations. Could be less if the "epsilon criteria" is hit.}

\item{learning_rate}{[\code{numeric(1)}]\cr
The step size used for gradient descent. Note: If the mse is not improving the step size
is shrinked by 20 percent.}

\item{beta_init}{[\code{numeric}]\cr
Initial vector of coefficients used as starting point for the gradient descent.}

\item{mse_eps}{[\code{numeric(1)}]\cr
Relativ improvement of the MSE. If this boundary is undershot, then the algorithm stops.}

\item{trace}{[\code{logical(1)}]\cr
Flag if the trace should be printed or not.}

\item{warnings}{[\code{logical(1)}]\cr
Flag to specify if warnings should be printed or not.}
}
\value{
List of parameter vector, the final mse, and a flag if the algorithm was stopped
  by the "epsilon criteria" or after the maximal iterations.
}
\description{
This function is just a wrapper around the \code{lmGradientDescent_internal()} function 
written in \code{C++}.
}
