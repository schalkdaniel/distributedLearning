% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/distributed_lm.R
\name{initializeDistributedModel}
\alias{initializeDistributedModel}
\title{Initialize a Distributed Linear Model}
\usage{
initializeDistributedModel(formula, model = "LinearModel",
  optimizer = "gradientDescent", out_dir = getwd(), files, epochs,
  learning_rate, mse_eps, save_all = FALSE, file_reader,
  overwrite = FALSE)
}
\arguments{
\item{formula}{[\code{formula}]\cr
Formula analog to the formula call in \code{lm}.}

\item{out_dir}{[\code{character(1)}]\cr
Direction for the output files.}

\item{files}{[\code{character}]\cr
Vector of file destinations. Each element must point to one dataset.}

\item{epochs}{[\code{integer(1)}]\cr
Number of maximal iterations. Could be less if the "epsilon criteria" is hit.}

\item{learning_rate}{[\code{numeric(1)}]\cr
The step size used for gradient descent. Note: If the mse is not improving the step size
is shrinked by 20 percent.}

\item{mse_eps}{[\code{numeric(1)}]\cr
Relativ improvement of the MSE. If this boundary is undershot, then the algorithm stops.}

\item{save_all}{[\code{logical(1)}]\cr
If set to TRUE, all updates are stored within the out_dir.}

\item{file_reader}{[\code{function}]\cr
Function to read the datasets specified in files.}

\item{overwrite}{[\code{logical(1)}]\cr
Flag to specify whether to overwrite an existing registry and model or not.}
}
\value{
List of parameter vector, the final mse, and a flag if the algorithm was stopped
  by the "epsilon criteria" or after the maximal iterations.
}
\description{
This function creates the files and file system required to train a linear model in a 
distributed fashion.
}
