% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/grad_descent_lm.R
\name{updateBeta}
\alias{updateBeta}
\title{Conduct one gradient descent step}
\usage{
updateBeta(formula, data, learning_rate = 0.05, actual_beta, mse_eps,
  trace = FALSE, warnings = FALSE)
}
\arguments{
\item{formula}{[\code{formula}]\cr
Formula analog to the formula call in \code{lm}.}

\item{data}{[\code{data.frame}]\cr
Data frame containing the data used for modeling.}

\item{learning_rate}{[\code{numeric(1)}]\cr
The step size used for gradient descent. Note: If the mse is not improving the step size
is shrinked by 20 percent.}

\item{actual_beta}{[\code{numeric}]\cr
Actual coefficient vector which should be updated.}

\item{mse_eps}{[\code{numeric(1)}]\cr
Relativ improvement of the MSE. If this boundary is undershot, then the algorithm stops.}

\item{trace}{[\code{logical(1)}]\cr
Flag if the trace should be printed or not.}

\item{warnings}{[\code{logical(1)}]\cr
Flag to specify if warnings should be printed or not.}
}
\value{
List of parameter vector, the final mse, and a flag if the algorithm was stopped
  by the "epsilon criteria" or after the maximal iterations.
}
\description{
This function is just a wrapper around the \code{updateBeta_internal()} function written
in \code{C++}.
}
